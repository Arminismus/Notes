---
annotation-target: D:/Obsidian/Armin's%20Notes/RL/Deep%20Reinforcement%20Learning/Miguel%20Morales%20-%20Grokking%20Deep%20Reinforcement%20Learning-Manning%20Publications%20(2020).pdf
---


>%%
>```annotation-json
>{"created":"2023-05-16T12:36:31.464Z","updated":"2023-05-16T12:36:31.464Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":79419,"end":79472},{"type":"TextQuoteSelector","exact":" complex sequential decision-making under uncertainty","prefix":"nners. These are all examples of","suffix":".I want to bring to your attenti"}]}]}
>```
>%%
>*%%PREFIX%%nners. These are all examples of%%HIGHLIGHT%% ==complex sequential decision-making under uncertainty== %%POSTFIX%%.I want to bring to your attenti*
>%%LINK%%[[#^vxpea8h7riq|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^vxpea8h7riq


>%%
>```annotation-json
>{"created":"2023-05-16T12:36:54.576Z","text":"There are also related problems such as sparse rewards, that make it difficult to determine what action led to the reward.","updated":"2023-05-16T12:36:54.576Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":80008,"end":80131},{"type":"TextQuoteSelector","exact":"The second word I used is sequential, and this one refers to the fact that in many problems, there are delayed consequences","prefix":" we learn from sampled feedback.","suffix":". In the coaching example, again"}]}]}
>```
>%%
>*%%PREFIX%%we learn from sampled feedback.%%HIGHLIGHT%% ==The second word I used is sequential, and this one refers to the fact that in many problems, there are delayed consequences== %%POSTFIX%%. In the coaching example, again*
>%%LINK%%[[#^3dp9tuwa01c|show annotation]]
>%%COMMENT%%
>There are also related problems such as sparse rewards, that make it difficult to determine what action led to the reward.
>%%TAGS%%
>
^3dp9tuwa01c


>%%
>```annotation-json
>{"created":"2023-05-16T12:39:54.732Z","updated":"2023-05-16T12:39:54.732Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":80580,"end":80719},{"type":"TextQuoteSelector","exact":"the word uncertainty refers to the fact that we don’t know the actual inner work-ings of the world to understand how our actions affect it;","prefix":"om sequential feedback.Finally, ","suffix":" everything is left to our inter"}]}]}
>```
>%%
>*%%PREFIX%%om sequential feedback.Finally,%%HIGHLIGHT%% ==the word uncertainty refers to the fact that we don’t know the actual inner work-ings of the world to understand how our actions affect it;== %%POSTFIX%%everything is left to our inter*
>%%LINK%%[[#^q0hrysoo5q|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^q0hrysoo5q


>%%
>```annotation-json
>{"created":"2023-05-16T12:40:11.602Z","text":"The need for exploration here arises as a way for us to be able to resolve the problem of uncertainty.","updated":"2023-05-16T12:40:11.602Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":81123,"end":81320},{"type":"TextQuoteSelector","exact":"This  uncertainty  gives rise  to  the  need  for  exploration.  Finding  the  appropriate  balance  between  exploration  and exploitation is challenging because we learn from evaluative feedback.","prefix":"enching  the  right  decision?  ","suffix":"In  this  chapter,  you’ll  lear"}]}]}
>```
>%%
>*%%PREFIX%%enching  the  right  decision?%%HIGHLIGHT%% ==This  uncertainty  gives rise  to  the  need  for  exploration.  Finding  the  appropriate  balance  between  exploration  and exploitation is challenging because we learn from evaluative feedback.== %%POSTFIX%%In  this  chapter,  you’ll  lear*
>%%LINK%%[[#^m5x88j9oiwj|show annotation]]
>%%COMMENT%%
>[[Exploration-Exploitation Trade-off|The need for exploration]]  here arises as a way for us to be able to resolve the problem of uncertainty.
>%%TAGS%%
>
^m5x88j9oiwj


>%%
>```annotation-json
>{"created":"2023-05-16T13:02:58.713Z","updated":"2023-05-16T13:02:58.713Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":82160,"end":82224},{"type":"TextQuoteSelector","exact":"The two core components in RL are the agent and the environment.","prefix":"onents of reinforcement learning","suffix":" The agent is the decision maker"}]}]}
>```
>%%
>*%%PREFIX%%onents of reinforcement learning%%HIGHLIGHT%% ==The two core components in RL are the agent and the environment.== %%POSTFIX%%The agent is the decision maker*
>%%LINK%%[[#^unzijbcpqlp|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^unzijbcpqlp


>%%
>```annotation-json
>{"created":"2023-05-16T13:03:23.895Z","updated":"2023-05-16T13:03:23.895Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":82383,"end":82457},{"type":"TextQuoteSelector","exact":"RL from other ML approaches is that the agent and the environment interact","prefix":"the fundamental distinctions of ","suffix":"; the agent attempts to influenc"}]}]}
>```
>%%
>*%%PREFIX%%the fundamental distinctions of%%HIGHLIGHT%% ==RL from other ML approaches is that the agent and the environment interact== %%POSTFIX%%; the agent attempts to influenc*
>%%LINK%%[[#^usjji5a49z|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^usjji5a49z


>%%
>```annotation-json
>{"created":"2023-05-16T13:04:31.549Z","text":"This assumption is not always true of course, there might be true randomness involved, but it will help us solve problems that we otherwise couldn't.","updated":"2023-05-16T13:04:31.549Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":84282,"end":84532},{"type":"TextQuoteSelector","exact":"we assume there’s a correlation between actions we take  and  what  happens  in  the  world.  It’s  just  that  it’s  so  complicated  to  understand  these relationships,  that  it’s  difficult  for  humans  to  connect  the  dots  with  certainty. ","prefix":"er,” in reinforcement learning, ","suffix":" But,  perhaps this is something"}]}]}
>```
>%%
>*%%PREFIX%%er,” in reinforcement learning,%%HIGHLIGHT%% ==we assume there’s a correlation between actions we take  and  what  happens  in  the  world.  It’s  just  that  it’s  so  complicated  to  understand  these relationships,  that  it’s  difficult  for  humans  to  connect  the  dots  with  certainty.== %%POSTFIX%%But,  perhaps this is something*
>%%LINK%%[[#^zofo66svxy|show annotation]]
>%%COMMENT%%
>This assumption is not always true of course, there might be true randomness involved, but it will help us solve problems that we otherwise couldn't.
>%%TAGS%%
>
^zofo66svxy


>%%
>```annotation-json
>{"created":"2023-05-16T13:06:15.408Z","text":"Agent means \"Actor\"","updated":"2023-05-16T13:06:15.408Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":87927,"end":88066},{"type":"TextQuoteSelector","exact":"For  now,  the  only  important  thing  for  you  to  know  about  agents  is  that  they  are  the  decision-makers in the RL big picture.","prefix":"hat are effective and efficient.","suffix":" They have internal components a"}]}]}
>```
>%%
>*%%PREFIX%%hat are effective and efficient.%%HIGHLIGHT%% ==For  now,  the  only  important  thing  for  you  to  know  about  agents  is  that  they  are  the  decision-makers in the RL big picture.== %%POSTFIX%%They have internal components a*
>%%LINK%%[[#^lke5l6hovr|show annotation]]
>%%COMMENT%%
>Agent means "Actor"
>%%TAGS%%
>
^lke5l6hovr


>%%
>```annotation-json
>{"created":"2023-05-16T13:25:29.549Z","updated":"2023-05-16T13:25:29.549Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":89705,"end":89752},{"type":"TextQuoteSelector","exact":"t least in the RL world, whether right or wrong","prefix":"n  MDP running under the hood (a","suffix":").The environment is represented"}]}]}
>```
>%%
>*%%PREFIX%%n  MDP running under the hood (a%%HIGHLIGHT%% ==t least in the RL world, whether right or wrong== %%POSTFIX%%).The environment is represented*
>%%LINK%%[[#^dzdiypdaysb|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^dzdiypdaysb


>%%
>```annotation-json
>{"created":"2023-05-16T13:25:51.440Z","updated":"2023-05-16T13:25:51.440Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":89754,"end":89940},{"type":"TextQuoteSelector","exact":"The environment is represented by a set of variables related to the problem. The combina-tion of all the possible values this set of variables can take is referred to as the state space.","prefix":" world, whether right or wrong).","suffix":" A state is a specific set of va"}]}]}
>```
>%%
>*%%PREFIX%%world, whether right or wrong).%%HIGHLIGHT%% ==The environment is represented by a set of variables related to the problem. The combina-tion of all the possible values this set of variables can take is referred to as the state space.== %%POSTFIX%%A state is a specific set of va*
>%%LINK%%[[#^dbbm5uiynt8|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^dbbm5uiynt8


>%%
>```annotation-json
>{"created":"2023-05-16T13:26:05.266Z","updated":"2023-05-16T13:26:05.266Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":90014,"end":90247},{"type":"TextQuoteSelector","exact":"Agents may or may not have access to the actual environment’s state; however, one way or another, agents can observe something from the environment. The set of variables the agent perceives at any given time is called an observation.","prefix":"ariables take at any given time.","suffix":"The  combination  of  all  possi"}]}]}
>```
>%%
>*%%PREFIX%%ariables take at any given time.%%HIGHLIGHT%% ==Agents may or may not have access to the actual environment’s state; however, one way or another, agents can observe something from the environment. The set of variables the agent perceives at any given time is called an observation.== %%POSTFIX%%The  combination  of  all  possi*
>%%LINK%%[[#^n05v9txw1wn|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^n05v9txw1wn


>%%
>```annotation-json
>{"created":"2023-05-16T13:26:36.304Z","text":"\nMeaning available actions are a function of the state.","updated":"2023-05-16T13:26:36.304Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":90788,"end":90879},{"type":"TextQuoteSelector","exact":"At every state, the environment makes available a set of actions the agent can choose from.","prefix":" uses the terms interchangeably.","suffix":" Often the set of actions is the"}]}]}
>```
>%%
>*%%PREFIX%%uses the terms interchangeably.%%HIGHLIGHT%% ==At every state, the environment makes available a set of actions the agent can choose from.== %%POSTFIX%%Often the set of actions is the*
>%%LINK%%[[#^zzr71c66jcs|show annotation]]
>%%COMMENT%%
>
>Meaning available actions are a function of the state.
>%%TAGS%%
>
^zzr71c66jcs


>%%
>```annotation-json
>{"created":"2023-05-16T13:27:07.261Z","updated":"2023-05-16T13:27:07.261Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":91101,"end":91259},{"type":"TextQuoteSelector","exact":"The environment may change states as a response to the agent’s action. The function that is responsible for this transition is called the transition function.","prefix":"ironment through these actions. ","suffix":"After a transition, the environm"}]}]}
>```
>%%
>*%%PREFIX%%ironment through these actions.%%HIGHLIGHT%% ==The environment may change states as a response to the agent’s action. The function that is responsible for this transition is called the transition function.== %%POSTFIX%%After a transition, the environm*
>%%LINK%%[[#^j9s9o0ff6w|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^j9s9o0ff6w


>%%
>```annotation-json
>{"created":"2023-05-16T13:27:24.411Z","updated":"2023-05-16T13:27:24.411Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":91259,"end":91561},{"type":"TextQuoteSelector","exact":"After a transition, the environment emits a new observation. The environment may also provide a reward signal as a response. The function responsible for this mapping is called the reward  function.  The  set  of  transition  and  reward  function  is  referred  to  as  the  model  of the environment.","prefix":" called the transition function.","suffix":"38 Chapter 2  I  Mathematical fo"}]}]}
>```
>%%
>*%%PREFIX%%called the transition function.%%HIGHLIGHT%% ==After a transition, the environment emits a new observation. The environment may also provide a reward signal as a response. The function responsible for this mapping is called the reward  function.  The  set  of  transition  and  reward  function  is  referred  to  as  the  model  of the environment.== %%POSTFIX%%38 Chapter 2  I  Mathematical fo*
>%%LINK%%[[#^om5yopmd7rb|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^om5yopmd7rb


>%%
>```annotation-json
>{"created":"2023-05-16T13:29:00.710Z","text":"This means the transitions can be probabilistic, given action a, we have a probability of transitioning to state s, which is defined for all states.","updated":"2023-05-16T13:29:00.710Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":93399,"end":93448},{"type":"TextQuoteSelector","exact":"BW  has  a  deterministic  transition  function: ","prefix":"nd an Right (action 1) action.  ","suffix":" a  Left  action  always  moves "}]}]}
>```
>%%
>*%%PREFIX%%nd an Right (action 1) action.%%HIGHLIGHT%% ==BW  has  a  deterministic  transition  function:== %%POSTFIX%%a  Left  action  always  moves*
>%%LINK%%[[#^8c8tobwrzl|show annotation]]
>%%COMMENT%%
>This means the transitions can be probabilistic, given action a, we have a probability of transitioning to state s, which is defined for all states.
>%%TAGS%%
>
^8c8tobwrzl


>%%
>```annotation-json
>{"created":"2023-05-16T13:31:10.615Z","updated":"2023-05-16T13:31:10.615Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":113293,"end":113699},{"type":"TextQuoteSelector","exact":"But why do you care about this? Well, in the environments we’ve explored so far it’s not that obvious, and it’s not that important. But because most RL (and DRL) agents are designed to take advantage of the Markov assumption, you must make sure you feed your agent the nec-essary variables to make it hold as tightly as possible (completely keeping the Markov assump-tion is impractical, perhaps impossible","prefix":" encoun-tered before that point.","suffix":").For  example,  if  you’re  des"}]}]}
>```
>%%
>*%%PREFIX%%encoun-tered before that point.%%HIGHLIGHT%% ==But why do you care about this? Well, in the environments we’ve explored so far it’s not that obvious, and it’s not that important. But because most RL (and DRL) agents are designed to take advantage of the Markov assumption, you must make sure you feed your agent the nec-essary variables to make it hold as tightly as possible (completely keeping the Markov assump-tion is impractical, perhaps impossible== %%POSTFIX%%).For  example,  if  you’re  des*
>%%LINK%%[[#^k7jyjh79kp|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^k7jyjh79kp


>%%
>```annotation-json
>{"created":"2023-05-16T13:31:28.798Z","text":"if the information is incomplete in this sense, the sequence of locations alone have correlations with the velocities, which violates markov.","updated":"2023-05-16T13:31:28.798Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":113945,"end":114098},{"type":"TextQuoteSelector","exact":"because you must assume the agent is memoryless, you need to feed the agent more information than just its x, y, z coordinates away from the landing pad.","prefix":"o land a spacecraft safely, and ","suffix":"But,  you  probably  know  that "}]}]}
>```
>%%
>*%%PREFIX%%o land a spacecraft safely, and%%HIGHLIGHT%% ==because you must assume the agent is memoryless, you need to feed the agent more information than just its x, y, z coordinates away from the landing pad.== %%POSTFIX%%But,  you  probably  know  that*
>%%LINK%%[[#^5h2yfd4jcx|show annotation]]
>%%COMMENT%%
>if the information is incomplete in this sense, the sequence of locations alone have correlations with the velocities, which violates markov.
>%%TAGS%%
>
^5h2yfd4jcx


>%%
>```annotation-json
>{"created":"2023-05-16T13:33:46.604Z","updated":"2023-05-16T13:33:46.604Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":101679,"end":101802},{"type":"TextQuoteSelector","exact":"The goal of this task is defined through the reward signal. The reward signal can be dense, sparse, or anything in between.","prefix":"mmonly has a well-defined task. ","suffix":" When you design environments, r"}]}]}
>```
>%%
>*%%PREFIX%%mmonly has a well-defined task.%%HIGHLIGHT%% ==The goal of this task is defined through the reward signal. The reward signal can be dense, sparse, or anything in between.== %%POSTFIX%%When you design environments, r*
>%%LINK%%[[#^lepu30fpp9|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^lepu30fpp9


>%%
>```annotation-json
>{"created":"2023-05-16T13:34:17.537Z","updated":"2023-05-16T13:34:17.537Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":101898,"end":102130},{"type":"TextQuoteSelector","exact":"The more dense, the more supervision the agent will have, and the faster the agent will learn, but the  more  bias  you’ll  inject  into  your  agent,  and  the  less  likely  the  agent  will  come  up  with unexpected  behaviors. ","prefix":"in your agent the way you want. ","suffix":" The  more  sparse,  the  less  "}]}]}
>```
>%%
>*%%PREFIX%%in your agent the way you want.%%HIGHLIGHT%% ==The more dense, the more supervision the agent will have, and the faster the agent will learn, but the  more  bias  you’ll  inject  into  your  agent,  and  the  less  likely  the  agent  will  come  up  with unexpected  behaviors.== %%POSTFIX%%The  more  sparse,  the  less*
>%%LINK%%[[#^f5krpedj776|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^f5krpedj776


>%%
>```annotation-json
>{"created":"2023-05-16T13:34:43.206Z","updated":"2023-05-16T13:34:43.206Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":102677,"end":102796},{"type":"TextQuoteSelector","exact":"Notice  that,  even  though  rewards  can  be  negative  values,  they  are still  called  rewards  in  the  RL  world.","prefix":" new observation  and  reward.  ","suffix":"  The  set  of  the  observation"}]}]}
>```
>%%
>*%%PREFIX%%new observation  and  reward.%%HIGHLIGHT%% ==Notice  that,  even  though  rewards  can  be  negative  values,  they  are still  called  rewards  in  the  RL  world.== %%POSTFIX%%The  set  of  the  observation*
>%%LINK%%[[#^n39uyla9s69|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^n39uyla9s69


>%%
>```annotation-json
>{"created":"2023-05-16T13:35:07.772Z","updated":"2023-05-16T13:35:07.772Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":103523,"end":103755},{"type":"TextQuoteSelector","exact":"The task the agent is trying to solve may or may not have a natural ending. Tasks that have a natural ending, such as a game, are called episodic tasks. Tasks that don’t, such as learning forward motion, are called continuing tasks.","prefix":"The engine of the environment 45","suffix":" The sequence of time steps from"}]}]}
>```
>%%
>*%%PREFIX%%The engine of the environment 45%%HIGHLIGHT%% ==The task the agent is trying to solve may or may not have a natural ending. Tasks that have a natural ending, such as a game, are called episodic tasks. Tasks that don’t, such as learning forward motion, are called continuing tasks.== %%POSTFIX%%The sequence of time steps from*
>%%LINK%%[[#^uwk7vmtj0pn|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^uwk7vmtj0pn


>%%
>```annotation-json
>{"created":"2023-05-18T16:20:43.190Z","updated":"2023-05-18T16:20:43.190Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":116458,"end":116697},{"type":"TextQuoteSelector","exact":" But  it’s  a  compatibility  convention  that  allows  for  all  algorithms  to converge to the same solution to make all actions available in a terminal state transition from that terminal state to itself with probability 1 and reward 0.","prefix":"+1, –1,  and  0,  respectively. ","suffix":" Otherwise, you run the risk of "}]}]}
>```
>%%
>*%%PREFIX%%+1, –1,  and  0,  respectively.%%HIGHLIGHT%% ==But  it’s  a  compatibility  convention  that  allows  for  all  algorithms  to converge to the same solution to make all actions available in a terminal state transition from that terminal state to itself with probability 1 and reward 0.== %%POSTFIX%%Otherwise, you run the risk of*
>%%LINK%%[[#^ri83lw7ppad|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ri83lw7ppad


>%%
>```annotation-json
>{"created":"2023-05-18T16:32:41.966Z","text":"Isn't this really prohibitive? as the agent becomes more adapted to its environment, like when the agent (like a human) can literally expand its skills, then the probabilities shift when a skill is enhanced, in favor of that skill.","updated":"2023-05-18T16:32:41.966Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":121504,"end":121729},{"type":"TextQuoteSelector","exact":"One key assumption of many RL (and DRL) algorithms is that this distribution is stationary. That is, while there may be highly stochastic transitions, the probability distribution may not change during training or evaluation.","prefix":"The engine of the environment 53","suffix":" Just as with the Markov assumpt"}]}]}
>```
>%%
>*%%PREFIX%%The engine of the environment 53%%HIGHLIGHT%% ==One key assumption of many RL (and DRL) algorithms is that this distribution is stationary. That is, while there may be highly stochastic transitions, the probability distribution may not change during training or evaluation.== %%POSTFIX%%Just as with the Markov assumpt*
>%%LINK%%[[#^w72nnbdzego|show annotation]]
>%%COMMENT%%
>Isn't this really prohibitive? as the agent becomes more adapted to its environment, like when the agent (like a human) can literally expand its skills, then the probabilities shift when a skill is enhanced, in favor of that skill.
>%%TAGS%%
>
^w72nnbdzego


>%%
>```annotation-json
>{"created":"2023-05-18T16:35:19.847Z","text":"the next state achieved given the action and state the action was taken in, could be a more result-driven way of giving reward to the agent. \n\nThis means that even if the agent made a \"bad decision\", since they got good results, just because the dynamics of the environment is stochastic, and hence they will give high value to it.\n\nThis effect will of course be dampened as the agent garners more and more experience to learning the dynamics of the environment, and see that this action would only result to that great outcome only a handful of times.","updated":"2023-05-18T16:35:19.847Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":124273,"end":124619},{"type":"TextQuoteSelector","exact":"R(s,a,s'), which is explicit, we could also use R(s,a), or even R(s), depending on our needs. Sometimes  rewarding  the  agent  based  on  state  is  what  we  need;  sometimes  it  makes  more sense to use the action and the state. However, the most explicit way to represent the reward function is to use a state, action, and next state triplet","prefix":"ction  can  be  represented  as ","suffix":". With that, we can compute the "}]}]}
>```
>%%
>*%%PREFIX%%ction  can  be  represented  as%%HIGHLIGHT%% ==R(s,a,s'), which is explicit, we could also use R(s,a), or even R(s), depending on our needs. Sometimes  rewarding  the  agent  based  on  state  is  what  we  need;  sometimes  it  makes  more sense to use the action and the state. However, the most explicit way to represent the reward function is to use a state, action, and next state triplet== %%POSTFIX%%. With that, we can compute the*
>%%LINK%%[[#^gsdm64hxant|show annotation]]
>%%COMMENT%%
>the next state achieved given the action and state the action was taken in, could be a more result-driven way of giving reward to the agent. 
>
>This means that even if the agent made a "bad decision", since they got good results, just because the dynamics of the environment is stochastic, and hence they will give high value to it.
>
>This effect will of course be dampened as the agent garners more and more experience to learning the dynamics of the environment, and see that this action would only result to that great outcome only a handful of times.
>%%TAGS%%
>
^gsdm64hxant


>%%
>```annotation-json
>{"created":"2023-05-18T16:39:23.302Z","updated":"2023-05-18T16:39:23.302Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":128004,"end":128022},{"type":"TextQuoteSelector","exact":"discretizing time.","prefix":"l clock syncing all parties and ","suffix":" Having a clock gives rise to a "}]}]}
>```
>%%
>*%%PREFIX%%l clock syncing all parties and%%HIGHLIGHT%% ==discretizing time.== %%POSTFIX%%Having a clock gives rise to a*
>%%LINK%%[[#^qfs20kn9b1i|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^qfs20kn9b1i
