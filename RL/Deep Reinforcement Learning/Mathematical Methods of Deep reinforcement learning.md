---
annotation-target: D:/Obsidian/Armin's%20Notes/RL/Deep%20Reinforcement%20Learning/Miguel%20Morales%20-%20Grokking%20Deep%20Reinforcement%20Learning-Manning%20Publications%20(2020).pdf
---


>%%
>```annotation-json
>{"created":"2023-05-16T12:36:31.464Z","updated":"2023-05-16T12:36:31.464Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":79419,"end":79472},{"type":"TextQuoteSelector","exact":" complex sequential decision-making under uncertainty","prefix":"nners. These are all examples of","suffix":".I want to bring to your attenti"}]}]}
>```
>%%
>*%%PREFIX%%nners. These are all examples of%%HIGHLIGHT%% ==complex sequential decision-making under uncertainty== %%POSTFIX%%.I want to bring to your attenti*
>%%LINK%%[[#^vxpea8h7riq|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^vxpea8h7riq


>%%
>```annotation-json
>{"created":"2023-05-16T12:36:54.576Z","text":"There are also related problems such as sparse rewards, that make it difficult to determine what action led to the reward.","updated":"2023-05-16T12:36:54.576Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":80008,"end":80131},{"type":"TextQuoteSelector","exact":"The second word I used is sequential, and this one refers to the fact that in many problems, there are delayed consequences","prefix":" we learn from sampled feedback.","suffix":". In the coaching example, again"}]}]}
>```
>%%
>*%%PREFIX%%we learn from sampled feedback.%%HIGHLIGHT%% ==The second word I used is sequential, and this one refers to the fact that in many problems, there are delayed consequences== %%POSTFIX%%. In the coaching example, again*
>%%LINK%%[[#^3dp9tuwa01c|show annotation]]
>%%COMMENT%%
>There are also related problems such as sparse rewards, that make it difficult to determine what action led to the reward.
>%%TAGS%%
>
^3dp9tuwa01c


>%%
>```annotation-json
>{"created":"2023-05-16T12:39:54.732Z","updated":"2023-05-16T12:39:54.732Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":80580,"end":80719},{"type":"TextQuoteSelector","exact":"the word uncertainty refers to the fact that we don’t know the actual inner work-ings of the world to understand how our actions affect it;","prefix":"om sequential feedback.Finally, ","suffix":" everything is left to our inter"}]}]}
>```
>%%
>*%%PREFIX%%om sequential feedback.Finally,%%HIGHLIGHT%% ==the word uncertainty refers to the fact that we don’t know the actual inner work-ings of the world to understand how our actions affect it;== %%POSTFIX%%everything is left to our inter*
>%%LINK%%[[#^q0hrysoo5q|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^q0hrysoo5q


>%%
>```annotation-json
>{"created":"2023-05-16T12:40:11.602Z","text":"The need for exploration here arises as a way for us to be able to resolve the problem of uncertainty.","updated":"2023-05-16T12:40:11.602Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":81123,"end":81320},{"type":"TextQuoteSelector","exact":"This  uncertainty  gives rise  to  the  need  for  exploration.  Finding  the  appropriate  balance  between  exploration  and exploitation is challenging because we learn from evaluative feedback.","prefix":"enching  the  right  decision?  ","suffix":"In  this  chapter,  you’ll  lear"}]}]}
>```
>%%
>*%%PREFIX%%enching  the  right  decision?%%HIGHLIGHT%% ==This  uncertainty  gives rise  to  the  need  for  exploration.  Finding  the  appropriate  balance  between  exploration  and exploitation is challenging because we learn from evaluative feedback.== %%POSTFIX%%In  this  chapter,  you’ll  lear*
>%%LINK%%[[#^m5x88j9oiwj|show annotation]]
>%%COMMENT%%
>[[Exploration-Exploitation Trade-off|The need for exploration]]  here arises as a way for us to be able to resolve the problem of uncertainty.
>%%TAGS%%
>
^m5x88j9oiwj


>%%
>```annotation-json
>{"created":"2023-05-16T13:02:58.713Z","updated":"2023-05-16T13:02:58.713Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":82160,"end":82224},{"type":"TextQuoteSelector","exact":"The two core components in RL are the agent and the environment.","prefix":"onents of reinforcement learning","suffix":" The agent is the decision maker"}]}]}
>```
>%%
>*%%PREFIX%%onents of reinforcement learning%%HIGHLIGHT%% ==The two core components in RL are the agent and the environment.== %%POSTFIX%%The agent is the decision maker*
>%%LINK%%[[#^unzijbcpqlp|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^unzijbcpqlp


>%%
>```annotation-json
>{"created":"2023-05-16T13:03:23.895Z","updated":"2023-05-16T13:03:23.895Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":82383,"end":82457},{"type":"TextQuoteSelector","exact":"RL from other ML approaches is that the agent and the environment interact","prefix":"the fundamental distinctions of ","suffix":"; the agent attempts to influenc"}]}]}
>```
>%%
>*%%PREFIX%%the fundamental distinctions of%%HIGHLIGHT%% ==RL from other ML approaches is that the agent and the environment interact== %%POSTFIX%%; the agent attempts to influenc*
>%%LINK%%[[#^usjji5a49z|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^usjji5a49z


>%%
>```annotation-json
>{"created":"2023-05-16T13:04:31.549Z","text":"This assumption is not always true of course, there might be true randomness involved, but it will help us solve problems that we otherwise couldn't.","updated":"2023-05-16T13:04:31.549Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":84282,"end":84532},{"type":"TextQuoteSelector","exact":"we assume there’s a correlation between actions we take  and  what  happens  in  the  world.  It’s  just  that  it’s  so  complicated  to  understand  these relationships,  that  it’s  difficult  for  humans  to  connect  the  dots  with  certainty. ","prefix":"er,” in reinforcement learning, ","suffix":" But,  perhaps this is something"}]}]}
>```
>%%
>*%%PREFIX%%er,” in reinforcement learning,%%HIGHLIGHT%% ==we assume there’s a correlation between actions we take  and  what  happens  in  the  world.  It’s  just  that  it’s  so  complicated  to  understand  these relationships,  that  it’s  difficult  for  humans  to  connect  the  dots  with  certainty.== %%POSTFIX%%But,  perhaps this is something*
>%%LINK%%[[#^zofo66svxy|show annotation]]
>%%COMMENT%%
>This assumption is not always true of course, there might be true randomness involved, but it will help us solve problems that we otherwise couldn't.
>%%TAGS%%
>
^zofo66svxy


>%%
>```annotation-json
>{"created":"2023-05-16T13:06:15.408Z","text":"Agent means \"Actor\"","updated":"2023-05-16T13:06:15.408Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":87927,"end":88066},{"type":"TextQuoteSelector","exact":"For  now,  the  only  important  thing  for  you  to  know  about  agents  is  that  they  are  the  decision-makers in the RL big picture.","prefix":"hat are effective and efficient.","suffix":" They have internal components a"}]}]}
>```
>%%
>*%%PREFIX%%hat are effective and efficient.%%HIGHLIGHT%% ==For  now,  the  only  important  thing  for  you  to  know  about  agents  is  that  they  are  the  decision-makers in the RL big picture.== %%POSTFIX%%They have internal components a*
>%%LINK%%[[#^lke5l6hovr|show annotation]]
>%%COMMENT%%
>Agent means "Actor"
>%%TAGS%%
>
^lke5l6hovr


>%%
>```annotation-json
>{"created":"2023-05-16T13:25:29.549Z","updated":"2023-05-16T13:25:29.549Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":89705,"end":89752},{"type":"TextQuoteSelector","exact":"t least in the RL world, whether right or wrong","prefix":"n  MDP running under the hood (a","suffix":").The environment is represented"}]}]}
>```
>%%
>*%%PREFIX%%n  MDP running under the hood (a%%HIGHLIGHT%% ==t least in the RL world, whether right or wrong== %%POSTFIX%%).The environment is represented*
>%%LINK%%[[#^dzdiypdaysb|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^dzdiypdaysb


>%%
>```annotation-json
>{"created":"2023-05-16T13:25:51.440Z","updated":"2023-05-16T13:25:51.440Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":89754,"end":89940},{"type":"TextQuoteSelector","exact":"The environment is represented by a set of variables related to the problem. The combina-tion of all the possible values this set of variables can take is referred to as the state space.","prefix":" world, whether right or wrong).","suffix":" A state is a specific set of va"}]}]}
>```
>%%
>*%%PREFIX%%world, whether right or wrong).%%HIGHLIGHT%% ==The environment is represented by a set of variables related to the problem. The combina-tion of all the possible values this set of variables can take is referred to as the state space.== %%POSTFIX%%A state is a specific set of va*
>%%LINK%%[[#^dbbm5uiynt8|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^dbbm5uiynt8


>%%
>```annotation-json
>{"created":"2023-05-16T13:26:05.266Z","updated":"2023-05-16T13:26:05.266Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":90014,"end":90247},{"type":"TextQuoteSelector","exact":"Agents may or may not have access to the actual environment’s state; however, one way or another, agents can observe something from the environment. The set of variables the agent perceives at any given time is called an observation.","prefix":"ariables take at any given time.","suffix":"The  combination  of  all  possi"}]}]}
>```
>%%
>*%%PREFIX%%ariables take at any given time.%%HIGHLIGHT%% ==Agents may or may not have access to the actual environment’s state; however, one way or another, agents can observe something from the environment. The set of variables the agent perceives at any given time is called an observation.== %%POSTFIX%%The  combination  of  all  possi*
>%%LINK%%[[#^n05v9txw1wn|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^n05v9txw1wn


>%%
>```annotation-json
>{"created":"2023-05-16T13:26:36.304Z","text":"\nMeaning available actions are a function of the state.","updated":"2023-05-16T13:26:36.304Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":90788,"end":90879},{"type":"TextQuoteSelector","exact":"At every state, the environment makes available a set of actions the agent can choose from.","prefix":" uses the terms interchangeably.","suffix":" Often the set of actions is the"}]}]}
>```
>%%
>*%%PREFIX%%uses the terms interchangeably.%%HIGHLIGHT%% ==At every state, the environment makes available a set of actions the agent can choose from.== %%POSTFIX%%Often the set of actions is the*
>%%LINK%%[[#^zzr71c66jcs|show annotation]]
>%%COMMENT%%
>
>Meaning available actions are a function of the state.
>%%TAGS%%
>
^zzr71c66jcs


>%%
>```annotation-json
>{"created":"2023-05-16T13:27:07.261Z","updated":"2023-05-16T13:27:07.261Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":91101,"end":91259},{"type":"TextQuoteSelector","exact":"The environment may change states as a response to the agent’s action. The function that is responsible for this transition is called the transition function.","prefix":"ironment through these actions. ","suffix":"After a transition, the environm"}]}]}
>```
>%%
>*%%PREFIX%%ironment through these actions.%%HIGHLIGHT%% ==The environment may change states as a response to the agent’s action. The function that is responsible for this transition is called the transition function.== %%POSTFIX%%After a transition, the environm*
>%%LINK%%[[#^j9s9o0ff6w|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^j9s9o0ff6w


>%%
>```annotation-json
>{"created":"2023-05-16T13:27:24.411Z","updated":"2023-05-16T13:27:24.411Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":91259,"end":91561},{"type":"TextQuoteSelector","exact":"After a transition, the environment emits a new observation. The environment may also provide a reward signal as a response. The function responsible for this mapping is called the reward  function.  The  set  of  transition  and  reward  function  is  referred  to  as  the  model  of the environment.","prefix":" called the transition function.","suffix":"38 Chapter 2  I  Mathematical fo"}]}]}
>```
>%%
>*%%PREFIX%%called the transition function.%%HIGHLIGHT%% ==After a transition, the environment emits a new observation. The environment may also provide a reward signal as a response. The function responsible for this mapping is called the reward  function.  The  set  of  transition  and  reward  function  is  referred  to  as  the  model  of the environment.== %%POSTFIX%%38 Chapter 2  I  Mathematical fo*
>%%LINK%%[[#^om5yopmd7rb|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^om5yopmd7rb


>%%
>```annotation-json
>{"created":"2023-05-16T13:29:00.710Z","text":"This means the transitions can be probabilistic, given action a, we have a probability of transitioning to state s, which is defined for all states.","updated":"2023-05-16T13:29:00.710Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":93399,"end":93448},{"type":"TextQuoteSelector","exact":"BW  has  a  deterministic  transition  function: ","prefix":"nd an Right (action 1) action.  ","suffix":" a  Left  action  always  moves "}]}]}
>```
>%%
>*%%PREFIX%%nd an Right (action 1) action.%%HIGHLIGHT%% ==BW  has  a  deterministic  transition  function:== %%POSTFIX%%a  Left  action  always  moves*
>%%LINK%%[[#^8c8tobwrzl|show annotation]]
>%%COMMENT%%
>This means the transitions can be probabilistic, given action a, we have a probability of transitioning to state s, which is defined for all states.
>%%TAGS%%
>
^8c8tobwrzl


>%%
>```annotation-json
>{"created":"2023-05-16T13:31:10.615Z","updated":"2023-05-16T13:31:10.615Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":113293,"end":113699},{"type":"TextQuoteSelector","exact":"But why do you care about this? Well, in the environments we’ve explored so far it’s not that obvious, and it’s not that important. But because most RL (and DRL) agents are designed to take advantage of the Markov assumption, you must make sure you feed your agent the nec-essary variables to make it hold as tightly as possible (completely keeping the Markov assump-tion is impractical, perhaps impossible","prefix":" encoun-tered before that point.","suffix":").For  example,  if  you’re  des"}]}]}
>```
>%%
>*%%PREFIX%%encoun-tered before that point.%%HIGHLIGHT%% ==But why do you care about this? Well, in the environments we’ve explored so far it’s not that obvious, and it’s not that important. But because most RL (and DRL) agents are designed to take advantage of the Markov assumption, you must make sure you feed your agent the nec-essary variables to make it hold as tightly as possible (completely keeping the Markov assump-tion is impractical, perhaps impossible== %%POSTFIX%%).For  example,  if  you’re  des*
>%%LINK%%[[#^k7jyjh79kp|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^k7jyjh79kp


>%%
>```annotation-json
>{"created":"2023-05-16T13:31:28.798Z","text":"if the information is incomplete in this sense, the sequence of locations alone have correlations with the velocities, which violates markov.","updated":"2023-05-16T13:31:28.798Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":113945,"end":114098},{"type":"TextQuoteSelector","exact":"because you must assume the agent is memoryless, you need to feed the agent more information than just its x, y, z coordinates away from the landing pad.","prefix":"o land a spacecraft safely, and ","suffix":"But,  you  probably  know  that "}]}]}
>```
>%%
>*%%PREFIX%%o land a spacecraft safely, and%%HIGHLIGHT%% ==because you must assume the agent is memoryless, you need to feed the agent more information than just its x, y, z coordinates away from the landing pad.== %%POSTFIX%%But,  you  probably  know  that*
>%%LINK%%[[#^5h2yfd4jcx|show annotation]]
>%%COMMENT%%
>if the information is incomplete in this sense, the sequence of locations alone have correlations with the velocities, which violates markov.
>%%TAGS%%
>
^5h2yfd4jcx


>%%
>```annotation-json
>{"created":"2023-05-16T13:33:46.604Z","updated":"2023-05-16T13:33:46.604Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":101679,"end":101802},{"type":"TextQuoteSelector","exact":"The goal of this task is defined through the reward signal. The reward signal can be dense, sparse, or anything in between.","prefix":"mmonly has a well-defined task. ","suffix":" When you design environments, r"}]}]}
>```
>%%
>*%%PREFIX%%mmonly has a well-defined task.%%HIGHLIGHT%% ==The goal of this task is defined through the reward signal. The reward signal can be dense, sparse, or anything in between.== %%POSTFIX%%When you design environments, r*
>%%LINK%%[[#^lepu30fpp9|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^lepu30fpp9


>%%
>```annotation-json
>{"created":"2023-05-16T13:34:17.537Z","updated":"2023-05-16T13:34:17.537Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":101898,"end":102130},{"type":"TextQuoteSelector","exact":"The more dense, the more supervision the agent will have, and the faster the agent will learn, but the  more  bias  you’ll  inject  into  your  agent,  and  the  less  likely  the  agent  will  come  up  with unexpected  behaviors. ","prefix":"in your agent the way you want. ","suffix":" The  more  sparse,  the  less  "}]}]}
>```
>%%
>*%%PREFIX%%in your agent the way you want.%%HIGHLIGHT%% ==The more dense, the more supervision the agent will have, and the faster the agent will learn, but the  more  bias  you’ll  inject  into  your  agent,  and  the  less  likely  the  agent  will  come  up  with unexpected  behaviors.== %%POSTFIX%%The  more  sparse,  the  less*
>%%LINK%%[[#^f5krpedj776|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^f5krpedj776


>%%
>```annotation-json
>{"created":"2023-05-16T13:34:43.206Z","updated":"2023-05-16T13:34:43.206Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":102677,"end":102796},{"type":"TextQuoteSelector","exact":"Notice  that,  even  though  rewards  can  be  negative  values,  they  are still  called  rewards  in  the  RL  world.","prefix":" new observation  and  reward.  ","suffix":"  The  set  of  the  observation"}]}]}
>```
>%%
>*%%PREFIX%%new observation  and  reward.%%HIGHLIGHT%% ==Notice  that,  even  though  rewards  can  be  negative  values,  they  are still  called  rewards  in  the  RL  world.== %%POSTFIX%%The  set  of  the  observation*
>%%LINK%%[[#^n39uyla9s69|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^n39uyla9s69


>%%
>```annotation-json
>{"created":"2023-05-16T13:35:07.772Z","updated":"2023-05-16T13:35:07.772Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":103523,"end":103755},{"type":"TextQuoteSelector","exact":"The task the agent is trying to solve may or may not have a natural ending. Tasks that have a natural ending, such as a game, are called episodic tasks. Tasks that don’t, such as learning forward motion, are called continuing tasks.","prefix":"The engine of the environment 45","suffix":" The sequence of time steps from"}]}]}
>```
>%%
>*%%PREFIX%%The engine of the environment 45%%HIGHLIGHT%% ==The task the agent is trying to solve may or may not have a natural ending. Tasks that have a natural ending, such as a game, are called episodic tasks. Tasks that don’t, such as learning forward motion, are called continuing tasks.== %%POSTFIX%%The sequence of time steps from*
>%%LINK%%[[#^uwk7vmtj0pn|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^uwk7vmtj0pn


>%%
>```annotation-json
>{"created":"2023-05-18T16:20:43.190Z","updated":"2023-05-18T16:20:43.190Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":116458,"end":116697},{"type":"TextQuoteSelector","exact":" But  it’s  a  compatibility  convention  that  allows  for  all  algorithms  to converge to the same solution to make all actions available in a terminal state transition from that terminal state to itself with probability 1 and reward 0.","prefix":"+1, –1,  and  0,  respectively. ","suffix":" Otherwise, you run the risk of "}]}]}
>```
>%%
>*%%PREFIX%%+1, –1,  and  0,  respectively.%%HIGHLIGHT%% ==But  it’s  a  compatibility  convention  that  allows  for  all  algorithms  to converge to the same solution to make all actions available in a terminal state transition from that terminal state to itself with probability 1 and reward 0.== %%POSTFIX%%Otherwise, you run the risk of*
>%%LINK%%[[#^ri83lw7ppad|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ri83lw7ppad


>%%
>```annotation-json
>{"created":"2023-05-18T16:32:41.966Z","text":"Isn't this really prohibitive? as the agent becomes more adapted to its environment, like when the agent (like a human) can literally expand its skills, then the probabilities shift when a skill is enhanced, in favor of that skill.","updated":"2023-05-18T16:32:41.966Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":121504,"end":121729},{"type":"TextQuoteSelector","exact":"One key assumption of many RL (and DRL) algorithms is that this distribution is stationary. That is, while there may be highly stochastic transitions, the probability distribution may not change during training or evaluation.","prefix":"The engine of the environment 53","suffix":" Just as with the Markov assumpt"}]}]}
>```
>%%
>*%%PREFIX%%The engine of the environment 53%%HIGHLIGHT%% ==One key assumption of many RL (and DRL) algorithms is that this distribution is stationary. That is, while there may be highly stochastic transitions, the probability distribution may not change during training or evaluation.== %%POSTFIX%%Just as with the Markov assumpt*
>%%LINK%%[[#^w72nnbdzego|show annotation]]
>%%COMMENT%%
>Isn't this really prohibitive? as the agent becomes more adapted to its environment, like when the agent (like a human) can literally expand its skills, then the probabilities shift when a skill is enhanced, in favor of that skill.
>%%TAGS%%
>
^w72nnbdzego


>%%
>```annotation-json
>{"created":"2023-05-18T16:35:19.847Z","text":"the next state achieved given the action and state the action was taken in, could be a more result-driven way of giving reward to the agent. \n\nThis means that even if the agent made a \"bad decision\", since they got good results, just because the dynamics of the environment is stochastic, and hence they will give high value to it.\n\nThis effect will of course be dampened as the agent garners more and more experience to learning the dynamics of the environment, and see that this action would only result to that great outcome only a handful of times.","updated":"2023-05-18T16:35:19.847Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":124273,"end":124619},{"type":"TextQuoteSelector","exact":"R(s,a,s'), which is explicit, we could also use R(s,a), or even R(s), depending on our needs. Sometimes  rewarding  the  agent  based  on  state  is  what  we  need;  sometimes  it  makes  more sense to use the action and the state. However, the most explicit way to represent the reward function is to use a state, action, and next state triplet","prefix":"ction  can  be  represented  as ","suffix":". With that, we can compute the "}]}]}
>```
>%%
>*%%PREFIX%%ction  can  be  represented  as%%HIGHLIGHT%% ==R(s,a,s'), which is explicit, we could also use R(s,a), or even R(s), depending on our needs. Sometimes  rewarding  the  agent  based  on  state  is  what  we  need;  sometimes  it  makes  more sense to use the action and the state. However, the most explicit way to represent the reward function is to use a state, action, and next state triplet== %%POSTFIX%%. With that, we can compute the*
>%%LINK%%[[#^gsdm64hxant|show annotation]]
>%%COMMENT%%
>the next state achieved given the action and state the action was taken in, could be a more result-driven way of giving reward to the agent. 
>
>This means that even if the agent made a "bad decision", since they got good results, just because the dynamics of the environment is stochastic, and hence they will give high value to it.
>
>This effect will of course be dampened as the agent garners more and more experience to learning the dynamics of the environment, and see that this action would only result to that great outcome only a handful of times.
>%%TAGS%%
>
^gsdm64hxant


>%%
>```annotation-json
>{"created":"2023-05-18T16:39:23.302Z","updated":"2023-05-18T16:39:23.302Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":128004,"end":128022},{"type":"TextQuoteSelector","exact":"discretizing time.","prefix":"l clock syncing all parties and ","suffix":" Having a clock gives rise to a "}]}]}
>```
>%%
>*%%PREFIX%%l clock syncing all parties and%%HIGHLIGHT%% ==discretizing time.== %%POSTFIX%%Having a clock gives rise to a*
>%%LINK%%[[#^qfs20kn9b1i|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^qfs20kn9b1i


>%%
>```annotation-json
>{"created":"2023-05-18T17:11:32.723Z","updated":"2023-05-18T17:11:32.723Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":147943,"end":148287},{"type":"TextQuoteSelector","exact":"You’ll  also  notice  that  when  an  agent  has  full  access  to  an  MDP,  there’s  no  uncertainty because  you  can  look  at  the  dynamics  and  rewards  and  calculate  expectations  directly. Calculating expectations directly means that there’s no need for exploration; that is, there’s no need to balance exploration and exploitation.","prefix":" (and DRL) algorithm originates.","suffix":" There’s no need for interaction"}]}]}
>```
>%%
>*%%PREFIX%%(and DRL) algorithm originates.%%HIGHLIGHT%% ==You’ll  also  notice  that  when  an  agent  has  full  access  to  an  MDP,  there’s  no  uncertainty because  you  can  look  at  the  dynamics  and  rewards  and  calculate  expectations  directly. Calculating expectations directly means that there’s no need for exploration; that is, there’s no need to balance exploration and exploitation.== %%POSTFIX%%There’s no need for interaction*
>%%LINK%%[[#^pgrrjy4rz4|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^pgrrjy4rz4


>%%
>```annotation-json
>{"created":"2023-05-18T17:13:35.763Z","updated":"2023-05-18T17:13:35.763Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":151356,"end":151433},{"type":"TextQuoteSelector","exact":"a plan—that is, a sequence of actions from the START state to the GOAL state.","prefix":"t must find is something called ","suffix":" But this doesn’t always work.H0"}]}]}
>```
>%%
>*%%PREFIX%%t must find is something called%%HIGHLIGHT%% ==a plan—that is, a sequence of actions from the START state to the GOAL state.== %%POSTFIX%%But this doesn’t always work.H0*
>%%LINK%%[[#^gle4d7rn2mh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^gle4d7rn2mh


>%%
>```annotation-json
>{"created":"2023-05-18T17:13:46.135Z","updated":"2023-05-18T17:13:46.135Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":152407,"end":152639},{"type":"TextQuoteSelector","exact":" This is a solid plan. But, in a stochastic environment, even the best of plans fail.Remember that in the FL environment, unintended action effects have even higher probability: 66.66% vs. 33.33%! You need to plan for the unexpected","prefix":"122 35 6 79 10 1113 14 15GOAL(1)","suffix":".A solid plan in the FL environm"}]}]}
>```
>%%
>*%%PREFIX%%122 35 6 79 10 1113 14 15GOAL(1)%%HIGHLIGHT%% ==This is a solid plan. But, in a stochastic environment, even the best of plans fail.Remember that in the FL environment, unintended action effects have even higher probability: 66.66% vs. 33.33%! You need to plan for the unexpected== %%POSTFIX%%.A solid plan in the FL environm*
>%%LINK%%[[#^5e1pmglsmkp|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^5e1pmglsmkp


>%%
>```annotation-json
>{"created":"2023-05-18T17:14:29.636Z","updated":"2023-05-18T17:14:29.636Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":153453,"end":153575},{"type":"TextQuoteSelector","exact":"What the agent needs to come up with is called a policy. Policies are universal plans; policies cover all possible states.","prefix":"ng immediate and long-term goals","suffix":" We need to plan for every possi"}]}]}
>```
>%%
>*%%PREFIX%%ng immediate and long-term goals%%HIGHLIGHT%% ==What the agent needs to come up with is called a policy. Policies are universal plans; policies cover all possible states.== %%POSTFIX%%We need to plan for every possi*
>%%LINK%%[[#^y0fcwlcmwmm|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^y0fcwlcmwmm


>%%
>```annotation-json
>{"created":"2023-05-18T17:14:43.692Z","updated":"2023-05-18T17:14:43.692Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":153619,"end":153785},{"type":"TextQuoteSelector","exact":"olicies can be stochastic or deterministic:  the  policy  can  return  action-probability  distributions  or  single  actions  for  a given  state  (or  observation).","prefix":"plan for every possible state. P","suffix":"  For  now,  we’re  working  wit"}]}]}
>```
>%%
>*%%PREFIX%%plan for every possible state. P%%HIGHLIGHT%% ==olicies can be stochastic or deterministic:  the  policy  can  return  action-probability  distributions  or  single  actions  for  a given  state  (or  observation).== %%POSTFIX%%For  now,  we’re  working  wit*
>%%LINK%%[[#^p9iqjszluzo|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^p9iqjszluzo


>%%
>```annotation-json
>{"created":"2023-05-18T17:15:21.599Z","updated":"2023-05-18T17:15:21.599Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":154106,"end":154435},{"type":"TextQuoteSelector","exact":" Because, even though we know how to act optimally, the environ-ment might send our agent backward to the hole even if we always select to go toward the goal. This is why returns aren’t enough. The agent is really looking to maximize the expected return; that means the return taking into account the environment’s stochasticity.","prefix":"hould I expect from this policy?","suffix":"Also, we need a method to automa"}]}]}
>```
>%%
>*%%PREFIX%%hould I expect from this policy?%%HIGHLIGHT%% ==Because, even though we know how to act optimally, the environ-ment might send our agent backward to the hole even if we always select to go toward the goal. This is why returns aren’t enough. The agent is really looking to maximize the expected return; that means the return taking into account the environment’s stochasticity.== %%POSTFIX%%Also, we need a method to automa*
>%%LINK%%[[#^63nicscgx74|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^63nicscgx74


>%%
>```annotation-json
>{"created":"2023-05-18T17:16:15.556Z","updated":"2023-05-18T17:16:15.556Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":155974,"end":156142},{"type":"TextQuoteSelector","exact":"Remember, policies can be stochastic: either directly over an action or a probability distribution over actions. We’ll expand on stochastic policies in later chapters.)","prefix":"for a given nonterminal state. (","suffix":"Here’s a sample policy.One immed"}]}]}
>```
>%%
>*%%PREFIX%%for a given nonterminal state. (%%HIGHLIGHT%% ==Remember, policies can be stochastic: either directly over an action or a probability distribution over actions. We’ll expand on stochastic policies in later chapters.)== %%POSTFIX%%Here’s a sample policy.One immed*
>%%LINK%%[[#^py68j6ibaw8|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^py68j6ibaw8


>%%
>```annotation-json
>{"created":"2023-05-18T17:18:54.840Z","text":"The transition probability \\($p(s',r|s,a)$\\) changes based on which last state and reward we stumbled to (we could, in theory, get to a state $s'$ but get a different reward (like having a distribution for the rewards of that state).\n\nfor \\($\\pi(a|s)$\\), we sum over actions.\n\nThere is a temporal relationship between the dynamics distribution and the policy distribution. the policy sits above the dynamics, because given an action, we have the dynamics, meaning the action is an input to the dynamics, so its probability must be multiplied first.(consider a deterministic policy as edge case and build to stochastic.)","updated":"2023-05-18T17:18:54.840Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":160777,"end":160791},{"type":"TextQuoteSelector","exact":" This equation","prefix":"ine them recursively like so.(8)","suffix":" is called the Bellman equation,"}]}]}
>```
>%%
>*%%PREFIX%%ine them recursively like so.(8)%%HIGHLIGHT%% ==This equation== %%POSTFIX%%is called the Bellman equation,*
>%%LINK%%[[#^1p7zihvi7hw|show annotation]]
>%%COMMENT%%
>The transition probability \($p(s',r|s,a)$\) changes based on which last state and reward we stumbled to (we could, in theory, get to a state $s'$ but get a different reward (like having a distribution for the rewards of that state).
>
>for \($\pi(a|s)$\), we sum over actions.
>
>There is a temporal relationship between the dynamics distribution and the policy distribution. the policy sits above the dynamics, because given an action, we have the dynamics, meaning the action is an input to the dynamics, so its probability must be multiplied first.(consider a deterministic policy as edge case and build to stochastic.)
>%%TAGS%%
>
^1p7zihvi7hw


>%%
>```annotation-json
>{"created":"2023-05-18T17:30:56.049Z","text":"We might not be aware which states we could go to from state s, so we need to know what actions are better.","updated":"2023-05-18T17:30:56.049Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":162410,"end":162592},{"type":"TextQuoteSelector","exact":"Think about it: if you don’t have an MDP, how can you decide what action to take merely by knowing the values of all states? V-functions don’t capture the dynamics of the environment","prefix":"we need action-value functions. ","suffix":". The Q-function, on the other h"}]}]}
>```
>%%
>*%%PREFIX%%we need action-value functions.%%HIGHLIGHT%% ==Think about it: if you don’t have an MDP, how can you decide what action to take merely by knowing the values of all states? V-functions don’t capture the dynamics of the environment== %%POSTFIX%%. The Q-function, on the other h*
>%%LINK%%[[#^s65z35qu3wd|show annotation]]
>%%COMMENT%%
>We might not be aware which states we could go to from state s, so we need to know what actions are better.
>%%TAGS%%
>
^s65z35qu3wd


>%%
>```annotation-json
>{"created":"2023-05-18T17:32:02.056Z","updated":"2023-05-18T17:32:02.056Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":165311,"end":165432},{"type":"TextQuoteSelector","exact":"An optimal policy is a policy that for every state can obtain expected returns greater than or equal to any other policy.","prefix":"onents are the best they can be.","suffix":" An optimal state-value function"}]}]}
>```
>%%
>*%%PREFIX%%onents are the best they can be.%%HIGHLIGHT%% ==An optimal policy is a policy that for every state can obtain expected returns greater than or equal to any other policy.== %%POSTFIX%%An optimal state-value function*
>%%LINK%%[[#^l98u48m6qkm|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^l98u48m6qkm


>%%
>```annotation-json
>{"created":"2023-05-18T17:32:19.379Z","updated":"2023-05-18T17:32:19.379Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":165433,"end":165548},{"type":"TextQuoteSelector","exact":"An optimal state-value function is a state-value function with the maximum value across all policies for all states","prefix":"n or equal to any other policy. ","suffix":". Likewise, an optimal action-va"}]}]}
>```
>%%
>*%%PREFIX%%n or equal to any other policy.%%HIGHLIGHT%% ==An optimal state-value function is a state-value function with the maximum value across all policies for all states== %%POSTFIX%%. Likewise, an optimal action-va*
>%%LINK%%[[#^7rivotwj6e8|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^7rivotwj6e8


>%%
>```annotation-json
>{"created":"2023-05-18T17:32:42.598Z","updated":"2023-05-18T17:32:42.598Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":165764,"end":165967},{"type":"TextQuoteSelector","exact":" but  notice  an  optimal advantage  function  would  be  equal  to  or  less  than  zero  for  all  state-action  pairs,  since  no action could have any advantage from the optimal state-value function.","prefix":"  follows  a  similar  pattern, ","suffix":"Also, notice that although there"}]}]}
>```
>%%
>*%%PREFIX%%follows  a  similar  pattern,%%HIGHLIGHT%% ==but  notice  an  optimal advantage  function  would  be  equal  to  or  less  than  zero  for  all  state-action  pairs,  since  no action could have any advantage from the optimal state-value function.== %%POSTFIX%%Also, notice that although there*
>%%LINK%%[[#^ffuzof7qo0q|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ffuzof7qo0q


>%%
>```annotation-json
>{"created":"2023-05-18T17:34:23.096Z","text":"This is where it means a lot to have a model of the environment. if we do have the model, the MDP that is, we can then know which states we could get to, and see which one has the highest value to choose the action to take us to that state. (but of course, multiple actions could take us to that state, so the action that takes us there with the highest probability?)","updated":"2023-05-18T17:34:23.096Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":166177,"end":166664},{"type":"TextQuoteSelector","exact":"You may also notice that if you had the optimal V-function, you could use the MDP to do a one-step search for the optimal Q-function and then use this to build the optimal policy. On the other hand, if you had the optimal Q-function, you don’t need the MDP at all. You could use the optimal Q-function to find the optimal V-function by merely taking the maxi-mum over the actions. And you could obtain the optimal policy using the optimal Q-function by taking the argmax over the actions","prefix":"i-mal action-advantage function.","suffix":".show Me The MaThThe Bellman opt"}]}]}
>```
>%%
>*%%PREFIX%%i-mal action-advantage function.%%HIGHLIGHT%% ==You may also notice that if you had the optimal V-function, you could use the MDP to do a one-step search for the optimal Q-function and then use this to build the optimal policy. On the other hand, if you had the optimal Q-function, you don’t need the MDP at all. You could use the optimal Q-function to find the optimal V-function by merely taking the maxi-mum over the actions. And you could obtain the optimal policy using the optimal Q-function by taking the argmax over the actions== %%POSTFIX%%.show Me The MaThThe Bellman opt*
>%%LINK%%[[#^dwjyd94zpye|show annotation]]
>%%COMMENT%%
>This is where it means a lot to have a model of the environment. if we do have the model, the MDP that is, we can then know which states we could get to, and see which one has the highest value to choose the action to take us to that state. (but of course, multiple actions could take us to that state, so the action that takes us there with the highest probability?)
>%%TAGS%%
>
^dwjyd94zpye


>%%
>```annotation-json
>{"created":"2023-05-18T17:37:24.834Z","updated":"2023-05-18T17:37:24.834Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":167944,"end":168200},{"type":"TextQuoteSelector","exact":"Iteratively computing the equations presented in the previous section is one of the most com-mon ways to solve a reinforcement learning problem and obtain optimal policies when the dynamics of the environment, the MDPs, are known. Let’s look at the methods","prefix":"ods for finding this objective. ","suffix":".Policy evaluation: Rating polic"}]}]}
>```
>%%
>*%%PREFIX%%ods for finding this objective.%%HIGHLIGHT%% ==Iteratively computing the equations presented in the previous section is one of the most com-mon ways to solve a reinforcement learning problem and obtain optimal policies when the dynamics of the environment, the MDPs, are known. Let’s look at the methods== %%POSTFIX%%.Policy evaluation: Rating polic*
>%%LINK%%[[#^upkpxd25ifl|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^upkpxd25ifl


>%%
>```annotation-json
>{"created":"2023-05-18T17:38:29.645Z","text":"\\($v_k$\\) is a table in the past, \\($v_{k+1}$\\) is a our present table. (or, we could use just one table)","updated":"2023-05-18T17:38:29.645Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":169062,"end":169242},{"type":"TextQuoteSelector","exact":" The policy-evaluation algorithm consist of the iterative approximation of the state-value function of the policy under evaluation. The algorithm converges as k approaches infinity","prefix":"he policy-evaluation equation(1)","suffix":".(2) Initialize v0(s) for all s "}]}]}
>```
>%%
>*%%PREFIX%%he policy-evaluation equation(1)%%HIGHLIGHT%% ==The policy-evaluation algorithm consist of the iterative approximation of the state-value function of the policy under evaluation. The algorithm converges as k approaches infinity== %%POSTFIX%%.(2) Initialize v0(s) for all s*
>%%LINK%%[[#^ny71r65tcfh|show annotation]]
>%%COMMENT%%
>\($v_k$\) is a table in the past, \($v_{k+1}$\) is a our present table. (or, we could use just one table)
>%%TAGS%%
>
^ny71r65tcfh


>%%
>```annotation-json
>{"created":"2023-05-18T17:44:54.989Z","updated":"2023-05-18T17:44:54.989Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":170485,"end":170927},{"type":"TextQuoteSelector","exact":"Also, it’s important to notice that the k’s here are iterations across estimates, but they’re not interactions with the environment. These aren’t episodes that the agent is out and about selecting actions and observing the environment. These aren’t time steps either. Instead, these are the iterations of the iterative policy-evaluation algorithm. Do a couple more of these esti-mates. The following table shows you the results you should get","prefix":"technique in RL (including DRL).","suffix":".H0 1 2START4 5 G63Initial calcu"}]}]}
>```
>%%
>*%%PREFIX%%technique in RL (including DRL).%%HIGHLIGHT%% ==Also, it’s important to notice that the k’s here are iterations across estimates, but they’re not interactions with the environment. These aren’t episodes that the agent is out and about selecting actions and observing the environment. These aren’t time steps either. Instead, these are the iterations of the iterative policy-evaluation algorithm. Do a couple more of these esti-mates. The following table shows you the results you should get== %%POSTFIX%%.H0 1 2START4 5 G63Initial calcu*
>%%LINK%%[[#^lp7fd831dyj|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^lp7fd831dyj


>%%
>```annotation-json
>{"created":"2023-05-18T17:47:47.722Z","text":"This is why it's called \"Policy Evaluation\". we are evaluating the what value the policy gets in expectation, given the state it is in.\n\nTo get the best policy's value, we use the optimality equation instead.","updated":"2023-05-18T17:47:47.722Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":174374,"end":174439},{"type":"TextQuoteSelector","exact":"See here how we use the policy pi to get the possible transitions","prefix":"te the state-value function.(6) ","suffix":".(7) Each transition tuple has a"}]}]}
>```
>%%
>*%%PREFIX%%te the state-value function.(6)%%HIGHLIGHT%% ==See here how we use the policy pi to get the possible transitions== %%POSTFIX%%.(7) Each transition tuple has a*
>%%LINK%%[[#^he564ywjks|show annotation]]
>%%COMMENT%%
>This is why it's called "Policy Evaluation". we are evaluating the what value the policy gets in expectation, given the state it is in.
>
>To get the best policy's value, we use the optimality equation instead.
>%%TAGS%%
>
^he564ywjks


>%%
>```annotation-json
>{"created":"2023-05-18T17:50:01.520Z","text":"Notice the states that are closer to the goal, have received higher values, since the agent has could stumble upon the goal more times.\n\nHowever note that we aren't doing any kind of simulation, we have the probabilities and we are simply calculating the numbers directly.","updated":"2023-05-18T17:50:01.520Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":176659,"end":176792},{"type":"TextQuoteSelector","exact":"After 218 interactions, policy evaluation converges to these values (using a 1e-10 minimum change in values as  a stopping condition)","prefix":"28 0.10380.4957 0.74170.0456(1) ","suffix":".State-value function of the ran"}]}]}
>```
>%%
>*%%PREFIX%%28 0.10380.4957 0.74170.0456(1)%%HIGHLIGHT%% ==After 218 interactions, policy evaluation converges to these values (using a 1e-10 minimum change in values as  a stopping condition)== %%POSTFIX%%.State-value function of the ran*
>%%LINK%%[[#^w06ld54qmoi|show annotation]]
>%%COMMENT%%
>Notice the states that are closer to the goal, have received higher values, since the agent has could stumble upon the goal more times.
>
>However note that we aren't doing any kind of simulation, we have the probabilities and we are simply calculating the numbers directly.
>%%TAGS%%
>
^w06ld54qmoi


>%%
>```annotation-json
>{"created":"2023-05-18T17:52:34.525Z","updated":"2023-05-18T17:52:34.525Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":179828,"end":180021},{"type":"TextQuoteSelector","exact":" To improve a policy, we use a state-value function and an MDP to get a one-step look-ahead and determine which of the actions lead to the highest value. This is the policy-improvement equation","prefix":" policy-improvement equation (1)","suffix":".(2) We obtain a new policy π� b"}]}]}
>```
>%%
>*%%PREFIX%%policy-improvement equation (1)%%HIGHLIGHT%% ==To improve a policy, we use a state-value function and an MDP to get a one-step look-ahead and determine which of the actions lead to the highest value. This is the policy-improvement equation== %%POSTFIX%%.(2) We obtain a new policy π� b*
>%%LINK%%[[#^lwyayokgo4q|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^lwyayokgo4q


>%%
>```annotation-json
>{"created":"2023-05-18T17:52:53.629Z","updated":"2023-05-18T17:52:53.629Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":180025,"end":180087},{"type":"TextQuoteSelector","exact":" We obtain a new policy π� by taking the highest-valued action","prefix":" policy-improvement equation.(2)","suffix":". (3) How do we get the highest-"}]}]}
>```
>%%
>*%%PREFIX%%policy-improvement equation.(2)%%HIGHLIGHT%% ==We obtain a new policy π� by taking the highest-valued action== %%POSTFIX%%. (3) How do we get the highest-*
>%%LINK%%[[#^4tixxhvem7k|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^4tixxhvem7k


>%%
>```annotation-json
>{"created":"2023-05-18T17:53:43.945Z","text":"So normally, if policy evaluation is done to completion, policy improvement can be done and then we are finished, we have a better policy.  which could be done in a loop.\n\nBut there is no need to do these iterations separately, as there might be many states that need to be visited to finish policy evaluation, without doing a single policy improvement step.  and many states could be useless to know their values since they might not be visited all that often in practice. \n\nSo, we can switch back and forth between policy improvement and evaluation.","updated":"2023-05-18T17:53:43.945Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":182776,"end":182957},{"type":"TextQuoteSelector","exact":"Now, even if we start with an adversarial policy designed to perform poorly, alternating over  policy  evaluation  and  improvement  would  still  end  up  with  an  optimal  policy","prefix":"s policy was good to begin with.","suffix":".  Want proof?  Let’s  do  it!  "}]}]}
>```
>%%
>*%%PREFIX%%s policy was good to begin with.%%HIGHLIGHT%% ==Now, even if we start with an adversarial policy designed to perform poorly, alternating over  policy  evaluation  and  improvement  would  still  end  up  with  an  optimal  policy== %%POSTFIX%%.  Want proof?  Let’s  do  it!*
>%%LINK%%[[#^zno7rxnzyfo|show annotation]]
>%%COMMENT%%
>So normally, if policy evaluation is done to completion, policy improvement can be done and then we are finished, we have a better policy.  which could be done in a loop.
>
>But there is no need to do these iterations separately, as there might be many states that need to be visited to finish policy evaluation, without doing a single policy improvement step.  and many states could be useless to know their values since they might not be visited all that often in practice. 
>
>So, we can switch back and forth between policy improvement and evaluation.
>%%TAGS%%
>
^zno7rxnzyfo


>%%
>```annotation-json
>{"created":"2023-05-18T17:57:39.583Z","updated":"2023-05-18T17:57:39.583Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":186637,"end":186841},{"type":"TextQuoteSelector","exact":"Notice how I use “an optimal policy,” but also use “the optimal state-value function.” This is not a coincidence or a poor choice of words; this is, in fact, a property that I’d like to high-light  again.","prefix":"ike to make about this sentence.","suffix":"  An  MDP  can  have  more  than"}]}]}
>```
>%%
>*%%PREFIX%%ike to make about this sentence.%%HIGHLIGHT%% ==Notice how I use “an optimal policy,” but also use “the optimal state-value function.” This is not a coincidence or a poor choice of words; this is, in fact, a property that I’d like to high-light  again.== %%POSTFIX%%An  MDP  can  have  more  than*
>%%LINK%%[[#^r3wk74zx4af|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^r3wk74zx4af


>%%
>```annotation-json
>{"created":"2023-05-18T17:57:45.665Z","updated":"2023-05-18T17:57:45.665Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":186843,"end":187012},{"type":"TextQuoteSelector","exact":"An  MDP  can  have  more  than  one  optimal  policy,  but  it  can  only  have  a  single optimal state-value function. It’s not too hard to wrap your head around that.","prefix":"I’d like to high-light  again.  ","suffix":"State-value  functions  are  col"}]}]}
>```
>%%
>*%%PREFIX%%I’d like to high-light  again.%%HIGHLIGHT%% ==An  MDP  can  have  more  than  one  optimal  policy,  but  it  can  only  have  a  single optimal state-value function. It’s not too hard to wrap your head around that.== %%POSTFIX%%State-value  functions  are  col*
>%%LINK%%[[#^a9uw6967ee6|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^a9uw6967ee6


>%%
>```annotation-json
>{"created":"2023-05-18T17:58:15.756Z","updated":"2023-05-18T17:58:15.756Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":187257,"end":187529},{"type":"TextQuoteSelector","exact":" However,  a  state-value  function  may  have actions that are equally valued for a given state; this includes the optimal state-value function. In this case, there could be multiple optimal policies, each optimal policy selecting a different, but equally valued, action.","prefix":"st  numbers  for  all  states). ","suffix":" Take a look: the FL environment"}]}]}
>```
>%%
>*%%PREFIX%%st  numbers  for  all  states).%%HIGHLIGHT%% ==However,  a  state-value  function  may  have actions that are equally valued for a given state; this includes the optimal state-value function. In this case, there could be multiple optimal policies, each optimal policy selecting a different, but equally valued, action.== %%POSTFIX%%Take a look: the FL environment*
>%%LINK%%[[#^j9z2ehddmek|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^j9z2ehddmek


>%%
>```annotation-json
>{"created":"2023-05-18T17:58:37.020Z","updated":"2023-05-18T17:58:37.020Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":187768,"end":187948},{"type":"TextQuoteSelector","exact":" I want to highlight that policy iteration is guaranteed to converge to the exact  optimal  policy:  the  mathematical  proof  shows  it  will  not  get  stuck  in  local  optima. ","prefix":"ting in state 6.As a final note,","suffix":"However,  as  a  practical  cons"}]}]}
>```
>%%
>*%%PREFIX%%ting in state 6.As a final note,%%HIGHLIGHT%% ==I want to highlight that policy iteration is guaranteed to converge to the exact  optimal  policy:  the  mathematical  proof  shows  it  will  not  get  stuck  in  local  optima.== %%POSTFIX%%However,  as  a  practical  cons*
>%%LINK%%[[#^wjr8tk0qowd|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^wjr8tk0qowd


>%%
>```annotation-json
>{"created":"2023-05-18T18:37:01.274Z","text":"The greedy part here is important, as it the policy itself is NOT greedy based on its value function, and this is what allows it to improve.","updated":"2023-05-18T18:37:01.274Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":197353,"end":197402},{"type":"TextQuoteSelector","exact":"a greedy policy from a value function and an MDP.","prefix":"ment is a method for extracting ","suffix":" Policy iteration consists of al"}]}]}
>```
>%%
>*%%PREFIX%%ment is a method for extracting%%HIGHLIGHT%% ==a greedy policy from a value function and an MDP.== %%POSTFIX%%Policy iteration consists of al*
>%%LINK%%[[#^6qmb9ca8exe|show annotation]]
>%%COMMENT%%
>The greedy part here is important, as it the policy itself is NOT greedy based on its value function, and this is what allows it to improve.
>%%TAGS%%
>
^6qmb9ca8exe


>%%
>```annotation-json
>{"created":"2023-05-18T18:39:19.093Z","updated":"2023-05-18T18:39:19.093Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":197932,"end":198337},{"type":"TextQuoteSelector","exact":"The more general view of these methods is generalized policy iteration, which describes the interaction of two processes to optimize policies: one moves value function estimates closer to  the  real  value  function  of  the  current  policy,  another  improves  the  current  policy  using its  value  function  estimates,  getting  progressively  better  and  better  policies  as  this  cycle continues","prefix":" policy-improvement phase early.","suffix":".By now, you• Know the objective"}]}]}
>```
>%%
>*%%PREFIX%%policy-improvement phase early.%%HIGHLIGHT%% ==The more general view of these methods is generalized policy iteration, which describes the interaction of two processes to optimize policies: one moves value function estimates closer to  the  real  value  function  of  the  current  policy,  another  improves  the  current  policy  using its  value  function  estimates,  getting  progressively  better  and  better  policies  as  this  cycle continues== %%POSTFIX%%.By now, you• Know the objective*
>%%LINK%%[[#^sh2txlvci8|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^sh2txlvci8


>%%
>```annotation-json
>{"created":"2023-05-18T18:40:23.303Z","updated":"2023-05-18T18:40:23.303Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":202799,"end":202907},{"type":"TextQuoteSelector","exact":"It boils down to deciding when to acquire knowl-edge and when to capitalize on knowledge previously learned.","prefix":"reinforcement learning problem. ","suffix":" It’s a challenge to know whethe"}]}]}
>```
>%%
>*%%PREFIX%%reinforcement learning problem.%%HIGHLIGHT%% ==It boils down to deciding when to acquire knowl-edge and when to capitalize on knowledge previously learned.== %%POSTFIX%%It’s a challenge to know whethe*
>%%LINK%%[[#^lyb0li7eg28|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^lyb0li7eg28


>%%
>```annotation-json
>{"created":"2023-05-18T18:42:00.363Z","updated":"2023-05-18T18:42:00.363Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":205682,"end":205856},{"type":"TextQuoteSelector","exact":"But, knowing an MDP in advance oversimplifies things, perhaps unrealistically. We can-not always assume we’ll know with precision how an environment will react to our actions","prefix":"act with the environment at all.","suffix":"—that’s not how the world works."}]}]}
>```
>%%
>*%%PREFIX%%act with the environment at all.%%HIGHLIGHT%% ==But, knowing an MDP in advance oversimplifies things, perhaps unrealistically. We can-not always assume we’ll know with precision how an environment will react to our actions== %%POSTFIX%%—that’s not how the world works.*
>%%LINK%%[[#^todvskufa3|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^todvskufa3


>%%
>```annotation-json
>{"created":"2023-05-18T18:48:14.124Z","updated":"2023-05-18T18:48:14.124Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":207234,"end":207356},{"type":"TextQuoteSelector","exact":"Multi-armed bandits (MAB) are a special case of an RL problem in which the size of the state space and horizon equal one. ","prefix":": Single-state decision problems","suffix":"A MAB has multiple actions, a si"}]}]}
>```
>%%
>*%%PREFIX%%: Single-state decision problems%%HIGHLIGHT%% ==Multi-armed bandits (MAB) are a special case of an RL problem in which the size of the state space and horizon equal one.== %%POSTFIX%%A MAB has multiple actions, a si*
>%%LINK%%[[#^n3ibcatimpk|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^n3ibcatimpk


>%%
>```annotation-json
>{"created":"2023-05-18T18:51:02.353Z","updated":"2023-05-18T18:51:02.353Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":211458,"end":211692},{"type":"TextQuoteSelector","exact":"To calculate this value, called total regret, we sum the per-episode difference of the true  expected  reward  of  the  optimal  action  and  the  true  expected  reward  of  the  selected action. Obviously, the lower the total regret","prefix":" rewards  across  all episodes. ","suffix":", the better. Notice I use the w"}]}]}
>```
>%%
>*%%PREFIX%%rewards  across  all episodes.%%HIGHLIGHT%% ==To calculate this value, called total regret, we sum the per-episode difference of the true  expected  reward  of  the  optimal  action  and  the  true  expected  reward  of  the  selected action. Obviously, the lower the total regret== %%POSTFIX%%, the better. Notice I use the w*
>%%LINK%%[[#^ye9e5z9wmoa|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ye9e5z9wmoa


>%%
>```annotation-json
>{"created":"2023-05-18T18:54:14.624Z","updated":"2023-05-18T18:54:14.624Z","document":{"title":"","link":[{"href":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9"}],"documentFingerprint":"97a9534828c344e2b3d7830dab729ac9"},"uri":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","target":[{"source":"urn:x-pdf:97a9534828c344e2b3d7830dab729ac9","selector":[{"type":"TextPositionSelector","start":213005,"end":213122},{"type":"TextQuoteSelector","exact":"Another approach to dealing with the exploration-exploitation dilemma is to be optimis-tic. Yep, your mom was right. ","prefix":"nsure the agent always explores?","suffix":"The family of optimistic explora"}]}]}
>```
>%%
>*%%PREFIX%%nsure the agent always explores?%%HIGHLIGHT%% ==Another approach to dealing with the exploration-exploitation dilemma is to be optimis-tic. Yep, your mom was right.== %%POSTFIX%%The family of optimistic explora*
>%%LINK%%[[#^d3rsevajqs|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^d3rsevajqs
